{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import *\n",
    "import pickle\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    config_filepath = \"config.json\"\n",
    "\n",
    "    with open(config_filepath, \"r\") as json_file:\n",
    "        config_data = json.load(json_file)\n",
    "\n",
    "    feats2level = config_data[\"feats2level\"]\n",
    "    allfeats_batch1 = config_data[\"allfeats_batch1\"]\n",
    "    allfeats_batch2 = config_data[\"allfeats_batch2\"]\n",
    "    features_to_load = config_data[\"features_to_load\"]\n",
    "    merged_data_pkl = config_data[\"merged_data_pkl\"]\n",
    "    base_folder = config_data[\"base_folder\"]\n",
    "    \n",
    "     # Load merged dataset\n",
    "    pickle_in = open(merged_data_pkl, \"rb\")\n",
    "    merged_dataset = pickle.load(pickle_in)\n",
    "    \n",
    "    # contrast = merged_dataset.merged_data[0]['contrast']\n",
    "    # # contrast = np.array(contrast)\n",
    "    # print(type(contrast))\n",
    "    # print(contrast.shape)\n",
    "\n",
    "    # print(len(contrast))\n",
    "    # print(contrast[0].shape)\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    # Perform train-test split\n",
    "    \n",
    "    train_dataset, test_dataset = train_test_split_by_subject(\n",
    "        merged_dataset,\n",
    "        feature_names=features_to_load,\n",
    "        feats2level = feats2level,\n",
    "        test_size=0.2,        # 20% for testing\n",
    "        random_state=29       # For reproducibility\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining Dataset: {len(train_dataset)} samples\")\n",
    "    print(f\"Testing Dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,      # Adjust based on your memory constraints\n",
    "        shuffle=True,       # Shuffle for training\n",
    "        num_workers=4       # Adjust based on your CPU cores\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=16,      # Adjust as needed\n",
    "        shuffle=False,      # No need to shuffle for testing\n",
    "        num_workers=4       # Adjust based on your CPU cores\n",
    "    )\n",
    "    \n",
    "    # Get NumPy arrays for scikit-learn (classification based on 'group_id')\n",
    "    X_train, y_train = train_dataset.get_numpy(label_column='group_id')\n",
    "    X_test, y_test = test_dataset.get_numpy(label_column='group_id')\n",
    "\n",
    "    print(\"\\nShape of X_train:\", X_train.shape)\n",
    "    print(\"Shape of y_train:\", y_train.shape)\n",
    "    print(\"Shape of X_test:\", X_test.shape)\n",
    "    print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "    print(\"\\nFirst 5 training labels:\", y_train[:5])\n",
    "    print(\"First 5 testing labels:\", y_test[:5])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from load_feat_pd import load_feat  # Ensure this module is in your PYTHONPATH\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import json\n",
    "\n",
    "def make_serializable(obj):\n",
    "    \"\"\"\n",
    "    Recursively converts non-serializable objects into serializable formats.\n",
    "    \n",
    "    Args:\n",
    "        obj: The object to serialize.\n",
    "    \n",
    "    Returns:\n",
    "        A serializable version of the object.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (np.ndarray, np.generic)):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (dict)):\n",
    "        return {k: make_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [make_serializable(v) for v in obj]\n",
    "    elif callable(obj):\n",
    "        return str(obj)  # Convert functions/methods to their string representation\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def nested_k_fold_cross_validation(\n",
    "    dataset: Dataset,\n",
    "    feature_names: List[str],\n",
    "    feats2level: dict,\n",
    "    model: BaseEstimator,\n",
    "    outer_k: int = 5,\n",
    "    inner_k: int = 3,\n",
    "    random_state: int = 42\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Performs nested k-fold cross-validation using the existing train_test_split_by_subject function\n",
    "    for the outer loop and GroupKFold for the inner loop.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The merged dataset (FeatureDataset instance).\n",
    "        feature_names (List[str]): List of feature names included in the dataset.\n",
    "        model (BaseEstimator): The machine learning model to train (must follow scikit-learn's estimator API).\n",
    "        outer_k (int): Number of outer folds.\n",
    "        inner_k (int): Number of inner folds.\n",
    "        random_state (int): Base random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List]: A dictionary containing performance metrics for each outer fold.\n",
    "    \"\"\"\n",
    "    # Convert merged data to DataFrame\n",
    "    merged_data = dataset.merged_data  # Assuming 'merged_data' is a list of dicts\n",
    "    df = pd.DataFrame(merged_data)\n",
    "\n",
    "    # Extract unique subject_ids and their genders\n",
    "    subjects_df = df[['subject_id', 'gender']].drop_duplicates()\n",
    "\n",
    "    # Drop subjects with missing gender\n",
    "    subjects_df = subjects_df.dropna(subset=['gender'])\n",
    "\n",
    "    # Ensure 'gender' is string type\n",
    "    subjects_df['gender'] = subjects_df['gender'].astype(str)\n",
    "\n",
    "    # Initialize StratifiedKFold for outer loop based on gender\n",
    "    outer_cv = StratifiedKFold(n_splits=outer_k, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Prepare data for outer loop\n",
    "    X_subjects_outer = subjects_df['subject_id']\n",
    "    y_subjects_outer = subjects_df['gender']\n",
    "\n",
    "    # Initialize a dictionary to store metrics\n",
    "    metrics = {\n",
    "        'accuracy': [],\n",
    "        'roc_auc': [],\n",
    "        'sensitivity': [],\n",
    "        'specificity': [],\n",
    "        'confusion_matrix': [],\n",
    "        'classification_report': []\n",
    "    }\n",
    "\n",
    "    # Outer Loop\n",
    "    for outer_fold, (train_subjects_idx, test_subjects_idx) in enumerate(outer_cv.split(X_subjects_outer, y_subjects_outer), 1):\n",
    "        print(f\"\\n=== Outer Fold {outer_fold} ===\")\n",
    "\n",
    "        # Extract outer train and test subject_ids\n",
    "        outer_train_subjects = subjects_df.iloc[train_subjects_idx]\n",
    "        outer_test_subjects = subjects_df.iloc[test_subjects_idx]\n",
    "\n",
    "        outer_train_ids = set(outer_train_subjects['subject_id'])\n",
    "        outer_test_ids = set(outer_test_subjects['subject_id'])\n",
    "\n",
    "        # Assign samples to outer train and test sets based on subject_id\n",
    "        outer_train_df = df[df['subject_id'].isin(outer_train_ids)].reset_index(drop=True)\n",
    "        outer_test_df = df[df['subject_id'].isin(outer_test_ids)].reset_index(drop=True)\n",
    "\n",
    "        # Create SubsetFeatureDataset instances\n",
    "        outer_train_dataset = SubsetFeatureDataset(outer_train_df, feature_names, feats2level)\n",
    "        outer_test_dataset = SubsetFeatureDataset(outer_test_df, feature_names, feats2level)\n",
    "\n",
    "        print(f\"Training subjects: {len(outer_train_ids)}\")\n",
    "        print(f\"Testing subjects: {len(outer_test_ids)}\")\n",
    "        print(f\"Total training samples: {len(outer_train_df)}\")\n",
    "        print(f\"Total testing samples: {len(outer_test_df)}\")\n",
    "\n",
    "        # Extract features and labels for outer train and test sets\n",
    "        X_outer_train, y_outer_train = outer_train_dataset.get_numpy(label_column='group_id')\n",
    "        X_outer_test, y_outer_test = outer_test_dataset.get_numpy(label_column='group_id')\n",
    "\n",
    "        # Initialize GroupKFold for inner loop based on subject_id\n",
    "        inner_cv = GroupKFold(n_splits=inner_k)\n",
    "\n",
    "        # Extract groups for inner loop\n",
    "        groups_outer_train = outer_train_df['subject_id'].values\n",
    "\n",
    "        best_score = -np.inf\n",
    "        best_model = None\n",
    "\n",
    "        # Inner Loop: Hyperparameter Tuning or Model Validation\n",
    "        for inner_fold, (inner_train_idx, inner_val_idx) in enumerate(inner_cv.split(X_outer_train, y_outer_train, groups=groups_outer_train), 1):\n",
    "            print(f\"  --- Inner Fold {inner_fold} ---\")\n",
    "\n",
    "            X_inner_train, X_inner_val = X_outer_train[inner_train_idx], X_outer_train[inner_val_idx]\n",
    "            y_inner_train, y_inner_val = y_outer_train[inner_train_idx], y_outer_train[inner_val_idx]  # Corrected here\n",
    "\n",
    "            # Clone the model to ensure independence\n",
    "            model_clone = clone(model)\n",
    "\n",
    "            # Train the model on inner training set\n",
    "            model_clone.fit(X_inner_train, y_inner_train)\n",
    "\n",
    "            # Evaluate on inner validation set\n",
    "            score = model_clone.score(X_inner_val, y_inner_val)\n",
    "            print(f\"    Inner Fold {inner_fold} Score: {score:.4f}\")\n",
    "\n",
    "            # Update best model if current model is better\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = clone(model_clone)\n",
    "\n",
    "        print(f\"  Best Inner Fold Score: {best_score:.4f}\")\n",
    "\n",
    "        # Train the best model on the entire outer training set\n",
    "        best_model.fit(X_outer_train, y_outer_train)\n",
    "\n",
    "        # Predict on the outer test set\n",
    "        y_pred = best_model.predict(X_outer_test)\n",
    "        if hasattr(best_model, \"predict_proba\"):\n",
    "            y_pred_proba = best_model.predict_proba(X_outer_test)[:, 1]\n",
    "        else:\n",
    "            # If model does not support predict_proba, use decision function or default probabilities\n",
    "            y_pred_proba = best_model.decision_function(X_outer_test)\n",
    "            # Ensure y_pred_proba is positive if necessary\n",
    "            if np.any(y_pred_proba < 0):\n",
    "                y_pred_proba = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min())\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        accuracy = accuracy_score(y_outer_test, y_pred)\n",
    "        metrics['accuracy'].append(accuracy)\n",
    "        print(f\"  Outer Fold {outer_fold} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Calculate ROC-AUC\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_outer_test, y_pred_proba)\n",
    "        except ValueError as e:\n",
    "            print(f\"    ROC-AUC Calculation Error: {e}\")\n",
    "            roc_auc = np.nan\n",
    "        metrics['roc_auc'].append(roc_auc)\n",
    "        print(f\"  Outer Fold {outer_fold} ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        # Generate Confusion Matrix\n",
    "        conf_matrix = confusion_matrix(y_outer_test, y_pred)\n",
    "        metrics['confusion_matrix'].append(conf_matrix.tolist())  # Convert ndarray to list\n",
    "        print(f\"  Outer Fold {outer_fold} Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "        # Generate Classification Report\n",
    "        class_report = classification_report(y_outer_test, y_pred, output_dict=True)\n",
    "        # Convert any numpy types within the dict to native Python types\n",
    "        class_report_serializable = make_serializable(class_report)\n",
    "        metrics['classification_report'].append(class_report_serializable)\n",
    "        print(f\"  Outer Fold {outer_fold} Classification Report:\\n{classification_report(y_outer_test, y_pred)}\")\n",
    "\n",
    "        # Calculate Sensitivity and Specificity\n",
    "        if conf_matrix.shape == (2, 2):\n",
    "            tn, fp, fn, tp = conf_matrix.ravel()\n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            metrics['sensitivity'].append(sensitivity)\n",
    "            metrics['specificity'].append(specificity)\n",
    "            print(f\"  Outer Fold {outer_fold} Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "            print(f\"  Outer Fold {outer_fold} Specificity: {specificity:.4f}\")\n",
    "        else:\n",
    "            print(\"  Confusion matrix is not binary. Skipping Sensitivity and Specificity calculation.\")\n",
    "            metrics['sensitivity'].append(None)\n",
    "            metrics['specificity'].append(None)\n",
    "\n",
    "    # Convert all metrics to serializable formats\n",
    "    serializable_metrics = make_serializable(metrics)\n",
    "\n",
    "    # Attempt to serialize and catch potential errors\n",
    "    try:\n",
    "        with open('nested_cv_metrics.json', 'w') as f:\n",
    "            json.dump(serializable_metrics, f, indent=4)\n",
    "        print(\"\\nNested K-Fold Cross-Validation completed and metrics saved to 'nested_cv_metrics.json'.\")\n",
    "    except TypeError as e:\n",
    "        print(f\"\\nSerialization Error: {e}\")\n",
    "        # Optionally, inspect the metrics to identify problematic entries\n",
    "        for key, value in metrics.items():\n",
    "            for idx, item in enumerate(value):\n",
    "                try:\n",
    "                    json.dumps(item)\n",
    "                except TypeError:\n",
    "                    print(f\"Non-serializable object found in '{key}' at index {idx}: {item}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from dataloader import *\n",
    "\n",
    "# Example Usage of Nested K-Fold Cross-Validation\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Load merged dataset\n",
    "    config_filepath = \"config.json\"\n",
    "\n",
    "    with open(config_filepath, \"r\") as json_file:\n",
    "        config_data = json.load(json_file)\n",
    "\n",
    "    feats2level = config_data[\"feats2level\"]\n",
    "    allfeats_batch1 = config_data[\"allfeats_batch1\"]\n",
    "    allfeats_batch2 = config_data[\"allfeats_batch2\"]\n",
    "    features_to_load = config_data[\"features_to_load\"]\n",
    "    merged_data_pkl = config_data[\"merged_data_pkl\"]\n",
    "    base_folder = config_data[\"base_folder\"]\n",
    "    \n",
    "     # Load merged dataset\n",
    "    pickle_in = open(merged_data_pkl, \"rb\")\n",
    "    merged_dataset = pickle.load(pickle_in)\n",
    "    print(f\"Loaded dataset of type: {type(merged_dataset)}\")\n",
    "    \n",
    "    seed = 7\n",
    "\n",
    "    # Perform Nested K-Fold Cross-Validation\n",
    "    # Initialize a classifier (e.g., RandomForestClassifier or LogisticRegression)\n",
    "    classifier = RandomForestClassifier(\n",
    "        n_estimators=100,          # Number of trees\n",
    "        max_depth=None,            # Maximum depth of each tree\n",
    "        min_samples_split=2,       # Minimum samples to split a node\n",
    "        min_samples_leaf=1,        # Minimum samples at a leaf node\n",
    "        random_state=seed,           # Seed for reproducibility\n",
    "        class_weight='balanced'    # Adjust weights inversely proportional to class frequencies\n",
    "    )\n",
    "\n",
    "    # Perform Nested Cross-Validation\n",
    "    metrics = nested_k_fold_cross_validation(\n",
    "        dataset=merged_dataset,\n",
    "        feature_names=features_to_load,\n",
    "        feats2level=feats2level,\n",
    "        model=classifier,\n",
    "        outer_k=5,\n",
    "        inner_k=5,\n",
    "        random_state=seed\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_averages = average_metrics('nested_cv_metrics.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yzhongenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
