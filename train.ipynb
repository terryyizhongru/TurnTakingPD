{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subjects: 52\n",
      "Training subjects: 41\n",
      "Testing subjects: 11\n",
      "Total training samples: 6143\n",
      "Total testing samples: 1666\n",
      "\n",
      "Gender distribution in test set:\n",
      "V    0.55102\n",
      "M    0.44898\n",
      "Name: gender, dtype: float64\n",
      "\n",
      "Group distribution in test set:\n",
      "21    0.540216\n",
      "22    0.459784\n",
      "Name: group_id, dtype: float64\n",
      "\n",
      "Training Dataset: 6143 samples\n",
      "Testing Dataset: 1666 samples\n",
      "\n",
      "Shape of X_train: (6143, 40)\n",
      "Shape of y_train: (6143,)\n",
      "Shape of X_test: (1666, 40)\n",
      "Shape of y_test: (1666,)\n",
      "\n",
      "First 5 training labels: [0 0 0 0 0]\n",
      "First 5 testing labels: [0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from dataloader import *\n",
    "import pickle\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    config_filepath = \"config.json\"\n",
    "\n",
    "    with open(config_filepath, \"r\") as json_file:\n",
    "        config_data = json.load(json_file)\n",
    "\n",
    "    feats2level = config_data[\"feats2level\"]\n",
    "    allfeats_batch1 = config_data[\"allfeats_batch1\"]\n",
    "    allfeats_batch2 = config_data[\"allfeats_batch2\"]\n",
    "    features_to_load = config_data[\"features_to_load\"]\n",
    "    merged_data_pkl = config_data[\"merged_data_pkl\"]\n",
    "    base_folder = config_data[\"base_folder\"]\n",
    "    \n",
    "     # Load merged dataset\n",
    "    pickle_in = open(merged_data_pkl, \"rb\")\n",
    "    merged_dataset = pickle.load(pickle_in)\n",
    "    \n",
    "    # contrast = merged_dataset.merged_data[0]['contrast']\n",
    "    # # contrast = np.array(contrast)\n",
    "    # print(type(contrast))\n",
    "    # print(contrast.shape)\n",
    "\n",
    "    # print(len(contrast))\n",
    "    # print(contrast[0].shape)\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "    # Perform train-test split\n",
    "    \n",
    "    train_dataset, test_dataset = train_test_split_by_subject(\n",
    "        merged_dataset,\n",
    "        feature_names=features_to_load,\n",
    "        feats2level = feats2level,\n",
    "        test_size=0.2,        # 20% for testing\n",
    "        random_state=525       # For reproducibility\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining Dataset: {len(train_dataset)} samples\")\n",
    "    print(f\"Testing Dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,      # Adjust based on your memory constraints\n",
    "        shuffle=True,       # Shuffle for training\n",
    "        num_workers=4       # Adjust based on your CPU cores\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=16,      # Adjust as needed\n",
    "        shuffle=False,      # No need to shuffle for testing\n",
    "        num_workers=4       # Adjust based on your CPU cores\n",
    "    )\n",
    "    \n",
    "    # Get NumPy arrays for scikit-learn (classification based on 'group_id')\n",
    "    X_train, y_train = train_dataset.get_numpy(label_column='group_id')\n",
    "    X_test, y_test = test_dataset.get_numpy(label_column='group_id')\n",
    "\n",
    "    print(\"\\nShape of X_train:\", X_train.shape)\n",
    "    print(\"Shape of y_train:\", y_train.shape)\n",
    "    print(\"Shape of X_test:\", X_test.shape)\n",
    "    print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "    print(\"\\nFirst 5 training labels:\", y_train[:5])\n",
    "    print(\"First 5 testing labels:\", y_test[:5])\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6747\n",
      "Confusion Matrix:\n",
      "[[744 156]\n",
      " [386 380]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.83      0.73       900\n",
      "           1       0.71      0.50      0.58       766\n",
      "\n",
      "    accuracy                           0.67      1666\n",
      "   macro avg       0.68      0.66      0.66      1666\n",
      "weighted avg       0.68      0.67      0.66      1666\n",
      "\n",
      "ROC-AUC: 0.7145\n",
      "Sensitivity (Recall): 0.4961\n",
      "Specificity: 0.8267\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100,          # Number of trees\n",
    "    max_depth=None,            # Maximum depth of each tree\n",
    "    min_samples_split=2,       # Minimum samples to split a node\n",
    "    min_samples_leaf=1,        # Minimum samples at a leaf node\n",
    "    random_state=42,           # Seed for reproducibility\n",
    "    class_weight='balanced'    # Adjust weights inversely proportional to class frequencies\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Generate Classification Report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Calculate ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Generate Confusion Matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "# Calculate Sensitivity and Specificity\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset of type: <class 'dataloader.FeatureDataset'>\n",
      "\n",
      "=== Outer Fold 1 ===\n",
      "Training subjects: 41\n",
      "Testing subjects: 11\n",
      "Total training samples: 6146\n",
      "Total testing samples: 1665\n",
      "  --- Inner Fold 1 ---\n",
      "    Inner Fold 1 Score: 0.4912\n",
      "  --- Inner Fold 2 ---\n",
      "    Inner Fold 2 Score: 0.6076\n",
      "  --- Inner Fold 3 ---\n",
      "    Inner Fold 3 Score: 0.6094\n",
      "  Best Inner Fold Score: 0.6094\n",
      "  Outer Fold 1 Accuracy: 0.5429\n",
      "  Outer Fold 1 ROC-AUC: 0.5171\n",
      "  Outer Fold 1 Confusion Matrix:\n",
      "[[746 311]\n",
      " [450 158]]\n",
      "  Outer Fold 1 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.71      0.66      1057\n",
      "           1       0.34      0.26      0.29       608\n",
      "\n",
      "    accuracy                           0.54      1665\n",
      "   macro avg       0.48      0.48      0.48      1665\n",
      "weighted avg       0.52      0.54      0.53      1665\n",
      "\n",
      "  Outer Fold 1 Sensitivity (Recall): 0.2599\n",
      "  Outer Fold 1 Specificity: 0.7058\n",
      "\n",
      "=== Outer Fold 2 ===\n",
      "Training subjects: 41\n",
      "Testing subjects: 11\n",
      "Total training samples: 6227\n",
      "Total testing samples: 1584\n",
      "  --- Inner Fold 1 ---\n",
      "    Inner Fold 1 Score: 0.4811\n",
      "  --- Inner Fold 2 ---\n",
      "    Inner Fold 2 Score: 0.6282\n",
      "  --- Inner Fold 3 ---\n",
      "    Inner Fold 3 Score: 0.5685\n",
      "  Best Inner Fold Score: 0.6282\n",
      "  Outer Fold 2 Accuracy: 0.6042\n",
      "  Outer Fold 2 ROC-AUC: 0.5805\n",
      "  Outer Fold 2 Confusion Matrix:\n",
      "[[636 187]\n",
      " [440 321]]\n",
      "  Outer Fold 2 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.77      0.67       823\n",
      "           1       0.63      0.42      0.51       761\n",
      "\n",
      "    accuracy                           0.60      1584\n",
      "   macro avg       0.61      0.60      0.59      1584\n",
      "weighted avg       0.61      0.60      0.59      1584\n",
      "\n",
      "  Outer Fold 2 Sensitivity (Recall): 0.4218\n",
      "  Outer Fold 2 Specificity: 0.7728\n",
      "\n",
      "=== Outer Fold 3 ===\n",
      "Training subjects: 42\n",
      "Testing subjects: 10\n",
      "Total training samples: 6297\n",
      "Total testing samples: 1514\n",
      "  --- Inner Fold 1 ---\n",
      "    Inner Fold 1 Score: 0.6192\n",
      "  --- Inner Fold 2 ---\n",
      "    Inner Fold 2 Score: 0.5844\n",
      "  --- Inner Fold 3 ---\n",
      "    Inner Fold 3 Score: 0.5593\n",
      "  Best Inner Fold Score: 0.6192\n",
      "  Outer Fold 3 Accuracy: 0.7272\n",
      "  Outer Fold 3 ROC-AUC: 0.7060\n",
      "  Outer Fold 3 Confusion Matrix:\n",
      "[[956 254]\n",
      " [159 145]]\n",
      "  Outer Fold 3 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.79      0.82      1210\n",
      "           1       0.36      0.48      0.41       304\n",
      "\n",
      "    accuracy                           0.73      1514\n",
      "   macro avg       0.61      0.63      0.62      1514\n",
      "weighted avg       0.76      0.73      0.74      1514\n",
      "\n",
      "  Outer Fold 3 Sensitivity (Recall): 0.4770\n",
      "  Outer Fold 3 Specificity: 0.7901\n",
      "\n",
      "=== Outer Fold 4 ===\n",
      "Training subjects: 42\n",
      "Testing subjects: 10\n",
      "Total training samples: 6286\n",
      "Total testing samples: 1525\n",
      "  --- Inner Fold 1 ---\n",
      "    Inner Fold 1 Score: 0.5233\n",
      "  --- Inner Fold 2 ---\n",
      "    Inner Fold 2 Score: 0.5923\n",
      "  --- Inner Fold 3 ---\n",
      "    Inner Fold 3 Score: 0.6425\n",
      "  Best Inner Fold Score: 0.6425\n",
      "  Outer Fold 4 Accuracy: 0.6295\n",
      "  Outer Fold 4 ROC-AUC: 0.6477\n",
      "  Outer Fold 4 Confusion Matrix:\n",
      "[[750 163]\n",
      " [402 210]]\n",
      "  Outer Fold 4 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.82      0.73       913\n",
      "           1       0.56      0.34      0.43       612\n",
      "\n",
      "    accuracy                           0.63      1525\n",
      "   macro avg       0.61      0.58      0.58      1525\n",
      "weighted avg       0.62      0.63      0.61      1525\n",
      "\n",
      "  Outer Fold 4 Sensitivity (Recall): 0.3431\n",
      "  Outer Fold 4 Specificity: 0.8215\n",
      "\n",
      "=== Outer Fold 5 ===\n",
      "Training subjects: 42\n",
      "Testing subjects: 10\n",
      "Total training samples: 6288\n",
      "Total testing samples: 1523\n",
      "  --- Inner Fold 1 ---\n",
      "    Inner Fold 1 Score: 0.5769\n",
      "  --- Inner Fold 2 ---\n",
      "    Inner Fold 2 Score: 0.4520\n",
      "  --- Inner Fold 3 ---\n",
      "    Inner Fold 3 Score: 0.7786\n",
      "  Best Inner Fold Score: 0.7786\n",
      "  Outer Fold 5 Accuracy: 0.5949\n",
      "  Outer Fold 5 ROC-AUC: 0.6550\n",
      "  Outer Fold 5 Confusion Matrix:\n",
      "[[636 127]\n",
      " [490 270]]\n",
      "  Outer Fold 5 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.83      0.67       763\n",
      "           1       0.68      0.36      0.47       760\n",
      "\n",
      "    accuracy                           0.59      1523\n",
      "   macro avg       0.62      0.59      0.57      1523\n",
      "weighted avg       0.62      0.59      0.57      1523\n",
      "\n",
      "  Outer Fold 5 Sensitivity (Recall): 0.3553\n",
      "  Outer Fold 5 Specificity: 0.8336\n",
      "\n",
      "Nested K-Fold Cross-Validation completed and metrics saved to 'nested_cv_metrics.json'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from load_feat_pd import load_feat  # Ensure this module is in your PYTHONPATH\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.base import BaseEstimator, clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import json\n",
    "\n",
    "def make_serializable(obj):\n",
    "    \"\"\"\n",
    "    Recursively converts non-serializable objects into serializable formats.\n",
    "    \n",
    "    Args:\n",
    "        obj: The object to serialize.\n",
    "    \n",
    "    Returns:\n",
    "        A serializable version of the object.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (np.ndarray, np.generic)):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (dict)):\n",
    "        return {k: make_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [make_serializable(v) for v in obj]\n",
    "    elif callable(obj):\n",
    "        return str(obj)  # Convert functions/methods to their string representation\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def nested_k_fold_cross_validation(\n",
    "    dataset: Dataset,\n",
    "    feature_names: List[str],\n",
    "    model: BaseEstimator,\n",
    "    outer_k: int = 5,\n",
    "    inner_k: int = 3,\n",
    "    random_state: int = 42\n",
    ") -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Performs nested k-fold cross-validation using the existing train_test_split_by_subject function\n",
    "    for the outer loop and GroupKFold for the inner loop.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The merged dataset (FeatureDataset instance).\n",
    "        feature_names (List[str]): List of feature names included in the dataset.\n",
    "        model (BaseEstimator): The machine learning model to train (must follow scikit-learn's estimator API).\n",
    "        outer_k (int): Number of outer folds.\n",
    "        inner_k (int): Number of inner folds.\n",
    "        random_state (int): Base random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List]: A dictionary containing performance metrics for each outer fold.\n",
    "    \"\"\"\n",
    "    # Convert merged data to DataFrame\n",
    "    merged_data = dataset.merged_data  # Assuming 'merged_data' is a list of dicts\n",
    "    df = pd.DataFrame(merged_data)\n",
    "\n",
    "    # Extract unique subject_ids and their genders\n",
    "    subjects_df = df[['subject_id', 'gender']].drop_duplicates()\n",
    "\n",
    "    # Drop subjects with missing gender\n",
    "    subjects_df = subjects_df.dropna(subset=['gender'])\n",
    "\n",
    "    # Ensure 'gender' is string type\n",
    "    subjects_df['gender'] = subjects_df['gender'].astype(str)\n",
    "\n",
    "    # Initialize StratifiedKFold for outer loop based on gender\n",
    "    outer_cv = StratifiedKFold(n_splits=outer_k, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # Prepare data for outer loop\n",
    "    X_subjects_outer = subjects_df['subject_id']\n",
    "    y_subjects_outer = subjects_df['gender']\n",
    "\n",
    "    # Initialize a dictionary to store metrics\n",
    "    metrics = {\n",
    "        'accuracy': [],\n",
    "        'roc_auc': [],\n",
    "        'sensitivity': [],\n",
    "        'specificity': [],\n",
    "        'confusion_matrix': [],\n",
    "        'classification_report': []\n",
    "    }\n",
    "\n",
    "    # Outer Loop\n",
    "    for outer_fold, (train_subjects_idx, test_subjects_idx) in enumerate(outer_cv.split(X_subjects_outer, y_subjects_outer), 1):\n",
    "        print(f\"\\n=== Outer Fold {outer_fold} ===\")\n",
    "\n",
    "        # Extract outer train and test subject_ids\n",
    "        outer_train_subjects = subjects_df.iloc[train_subjects_idx]\n",
    "        outer_test_subjects = subjects_df.iloc[test_subjects_idx]\n",
    "\n",
    "        outer_train_ids = set(outer_train_subjects['subject_id'])\n",
    "        outer_test_ids = set(outer_test_subjects['subject_id'])\n",
    "\n",
    "        # Assign samples to outer train and test sets based on subject_id\n",
    "        outer_train_df = df[df['subject_id'].isin(outer_train_ids)].reset_index(drop=True)\n",
    "        outer_test_df = df[df['subject_id'].isin(outer_test_ids)].reset_index(drop=True)\n",
    "\n",
    "        # Create SubsetFeatureDataset instances\n",
    "        outer_train_dataset = SubsetFeatureDataset(outer_train_df, feature_names)\n",
    "        outer_test_dataset = SubsetFeatureDataset(outer_test_df, feature_names)\n",
    "\n",
    "        print(f\"Training subjects: {len(outer_train_ids)}\")\n",
    "        print(f\"Testing subjects: {len(outer_test_ids)}\")\n",
    "        print(f\"Total training samples: {len(outer_train_df)}\")\n",
    "        print(f\"Total testing samples: {len(outer_test_df)}\")\n",
    "\n",
    "        # Extract features and labels for outer train and test sets\n",
    "        X_outer_train, y_outer_train = outer_train_dataset.get_numpy(label_column='group_id')\n",
    "        X_outer_test, y_outer_test = outer_test_dataset.get_numpy(label_column='group_id')\n",
    "\n",
    "        # Initialize GroupKFold for inner loop based on subject_id\n",
    "        inner_cv = GroupKFold(n_splits=inner_k)\n",
    "\n",
    "        # Extract groups for inner loop\n",
    "        groups_outer_train = outer_train_df['subject_id'].values\n",
    "\n",
    "        best_score = -np.inf\n",
    "        best_model = None\n",
    "\n",
    "        # Inner Loop: Hyperparameter Tuning or Model Validation\n",
    "        for inner_fold, (inner_train_idx, inner_val_idx) in enumerate(inner_cv.split(X_outer_train, y_outer_train, groups=groups_outer_train), 1):\n",
    "            print(f\"  --- Inner Fold {inner_fold} ---\")\n",
    "\n",
    "            X_inner_train, X_inner_val = X_outer_train[inner_train_idx], X_outer_train[inner_val_idx]\n",
    "            y_inner_train, y_inner_val = y_outer_train[inner_train_idx], y_outer_train[inner_val_idx]  # Corrected here\n",
    "\n",
    "            # Clone the model to ensure independence\n",
    "            model_clone = clone(model)\n",
    "\n",
    "            # Train the model on inner training set\n",
    "            model_clone.fit(X_inner_train, y_inner_train)\n",
    "\n",
    "            # Evaluate on inner validation set\n",
    "            score = model_clone.score(X_inner_val, y_inner_val)\n",
    "            print(f\"    Inner Fold {inner_fold} Score: {score:.4f}\")\n",
    "\n",
    "            # Update best model if current model is better\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_model = clone(model_clone)\n",
    "\n",
    "        print(f\"  Best Inner Fold Score: {best_score:.4f}\")\n",
    "\n",
    "        # Train the best model on the entire outer training set\n",
    "        best_model.fit(X_outer_train, y_outer_train)\n",
    "\n",
    "        # Predict on the outer test set\n",
    "        y_pred = best_model.predict(X_outer_test)\n",
    "        if hasattr(best_model, \"predict_proba\"):\n",
    "            y_pred_proba = best_model.predict_proba(X_outer_test)[:, 1]\n",
    "        else:\n",
    "            # If model does not support predict_proba, use decision function or default probabilities\n",
    "            y_pred_proba = best_model.decision_function(X_outer_test)\n",
    "            # Ensure y_pred_proba is positive if necessary\n",
    "            if np.any(y_pred_proba < 0):\n",
    "                y_pred_proba = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min())\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        accuracy = accuracy_score(y_outer_test, y_pred)\n",
    "        metrics['accuracy'].append(accuracy)\n",
    "        print(f\"  Outer Fold {outer_fold} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Calculate ROC-AUC\n",
    "        try:\n",
    "            roc_auc = roc_auc_score(y_outer_test, y_pred_proba)\n",
    "        except ValueError as e:\n",
    "            print(f\"    ROC-AUC Calculation Error: {e}\")\n",
    "            roc_auc = np.nan\n",
    "        metrics['roc_auc'].append(roc_auc)\n",
    "        print(f\"  Outer Fold {outer_fold} ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        # Generate Confusion Matrix\n",
    "        conf_matrix = confusion_matrix(y_outer_test, y_pred)\n",
    "        metrics['confusion_matrix'].append(conf_matrix.tolist())  # Convert ndarray to list\n",
    "        print(f\"  Outer Fold {outer_fold} Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "        # Generate Classification Report\n",
    "        class_report = classification_report(y_outer_test, y_pred, output_dict=True)\n",
    "        # Convert any numpy types within the dict to native Python types\n",
    "        class_report_serializable = make_serializable(class_report)\n",
    "        metrics['classification_report'].append(class_report_serializable)\n",
    "        print(f\"  Outer Fold {outer_fold} Classification Report:\\n{classification_report(y_outer_test, y_pred)}\")\n",
    "\n",
    "        # Calculate Sensitivity and Specificity\n",
    "        if conf_matrix.shape == (2, 2):\n",
    "            tn, fp, fn, tp = conf_matrix.ravel()\n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            metrics['sensitivity'].append(sensitivity)\n",
    "            metrics['specificity'].append(specificity)\n",
    "            print(f\"  Outer Fold {outer_fold} Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "            print(f\"  Outer Fold {outer_fold} Specificity: {specificity:.4f}\")\n",
    "        else:\n",
    "            print(\"  Confusion matrix is not binary. Skipping Sensitivity and Specificity calculation.\")\n",
    "            metrics['sensitivity'].append(None)\n",
    "            metrics['specificity'].append(None)\n",
    "\n",
    "    # Convert all metrics to serializable formats\n",
    "    serializable_metrics = make_serializable(metrics)\n",
    "\n",
    "    # Attempt to serialize and catch potential errors\n",
    "    try:\n",
    "        with open('nested_cv_metrics.json', 'w') as f:\n",
    "            json.dump(serializable_metrics, f, indent=4)\n",
    "        print(\"\\nNested K-Fold Cross-Validation completed and metrics saved to 'nested_cv_metrics.json'.\")\n",
    "    except TypeError as e:\n",
    "        print(f\"\\nSerialization Error: {e}\")\n",
    "        # Optionally, inspect the metrics to identify problematic entries\n",
    "        for key, value in metrics.items():\n",
    "            for idx, item in enumerate(value):\n",
    "                try:\n",
    "                    json.dumps(item)\n",
    "                except TypeError:\n",
    "                    print(f\"Non-serializable object found in '{key}' at index {idx}: {item}\")\n",
    "\n",
    "\n",
    "# Example Usage of Nested K-Fold Cross-Validation\n",
    "if __name__ == '__main__':\n",
    "    import pickle\n",
    "\n",
    "    # Load merged dataset\n",
    "    with open(\"merged_dataset.pkl\", \"rb\") as f:\n",
    "        merged_dataset = pickle.load(f)\n",
    "    print(f\"Loaded dataset of type: {type(merged_dataset)}\")\n",
    "\n",
    "    # Perform Nested K-Fold Cross-Validation\n",
    "    # Initialize a classifier (e.g., RandomForestClassifier or LogisticRegression)\n",
    "    classifier = RandomForestClassifier(\n",
    "        n_estimators=100,          # Number of trees\n",
    "        max_depth=None,            # Maximum depth of each tree\n",
    "        min_samples_split=2,       # Minimum samples to split a node\n",
    "        min_samples_leaf=1,        # Minimum samples at a leaf node\n",
    "        random_state=42,           # Seed for reproducibility\n",
    "        class_weight='balanced'    # Adjust weights inversely proportional to class frequencies\n",
    "    )\n",
    "\n",
    "    # Perform Nested Cross-Validation\n",
    "    metrics = nested_k_fold_cross_validation(\n",
    "        dataset=merged_dataset,\n",
    "        feature_names=features_to_load,\n",
    "        model=classifier,\n",
    "        outer_k=5,\n",
    "        inner_k=3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'float' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 98\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m averages\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# 调用average_metrics函数，传入JSON文件路径\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     metrics_averages \u001b[38;5;241m=\u001b[39m \u001b[43maverage_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnested_cv_metrics.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# 如果需要，可以进一步处理metrics_averages字典\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# 例如，保存到另一个文件或进行可视化\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 52\u001b[0m, in \u001b[0;36maverage_metrics\u001b[0;34m(json_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m     values \u001b[38;5;241m=\u001b[39m [m[key] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m[key], (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m))]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m         aggregated_report[key] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 52\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m v \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m v[key]]),\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m v \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m v[key]]),\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1-score\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1-score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m v \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1-score\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m v[key]]),\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean([v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m v \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m v[key]])),\n\u001b[1;32m     56\u001b[0m         }\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# 每个类别的指标\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     values \u001b[38;5;241m=\u001b[39m [m[key] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m[key], \u001b[38;5;28mdict\u001b[39m)]\n",
      "Cell \u001b[0;32mIn[15], line 52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m     values \u001b[38;5;241m=\u001b[39m [m[key] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m[key], (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m))]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m         aggregated_report[key] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 52\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m v \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m]),\n\u001b[1;32m     53\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m v \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m v[key]]),\n\u001b[1;32m     54\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1-score\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean([v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1-score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m v \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1-score\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m v[key]]),\n\u001b[1;32m     55\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean([v[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m v \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupport\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m v[key]])),\n\u001b[1;32m     56\u001b[0m         }\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# 每个类别的指标\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     values \u001b[38;5;241m=\u001b[39m [m[key] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m[key], \u001b[38;5;28mdict\u001b[39m)]\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'float' is not iterable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def make_serializable(obj):\n",
    "    \"\"\"\n",
    "    递归地将不可序列化的对象转换为可序列化格式。\n",
    "    \n",
    "    Args:\n",
    "        obj: 需要序列化的对象。\n",
    "    \n",
    "    Returns:\n",
    "        可序列化的对象。\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (np.ndarray, np.generic)):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: make_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [make_serializable(v) for v in obj]\n",
    "    elif callable(obj):\n",
    "        return str(obj)  # 将函数或方法转换为字符串表示\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def average_metrics(json_path: str) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    计算嵌套交叉验证保存的metrics的平均值和标准差。\n",
    "    \n",
    "    Args:\n",
    "        json_path (str): 存储metrics的JSON文件路径。\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, any]: 包含各个指标平均值和标准差的字典。\n",
    "    \"\"\"\n",
    "    # 读取JSON文件\n",
    "    with open(json_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    # 初始化用于存储平均值和标准差的字典\n",
    "    averages = {}\n",
    "    stds = {}\n",
    "    \n",
    "    # 简单数值指标\n",
    "    simple_metrics = ['accuracy', 'roc_auc', 'sensitivity', 'specificity']\n",
    "    \n",
    "    for metric in simple_metrics:\n",
    "        values = metrics.get(metric, [])\n",
    "        # 转换为numpy数组，忽略None或NaN\n",
    "        values = np.array([v for v in values if v is not None and not np.isnan(v)])\n",
    "        if len(values) > 0:\n",
    "            averages[metric] = np.mean(values)\n",
    "            stds[metric] = np.std(values)\n",
    "        else:\n",
    "            averages[metric] = None\n",
    "            stds[metric] = None\n",
    "    \n",
    "    # 处理混淆矩阵，计算平均混淆矩阵\n",
    "    if 'confusion_matrix' in metrics and len(metrics['confusion_matrix']) > 0:\n",
    "        conf_matrices = [np.array(cm) for cm in metrics['confusion_matrix']]\n",
    "        avg_conf_matrix = np.mean(conf_matrices, axis=0)\n",
    "        averages['confusion_matrix'] = avg_conf_matrix.tolist()\n",
    "    else:\n",
    "        averages['confusion_matrix'] = None\n",
    "    \n",
    "    # 处理分类报告，聚合每个类别的指标\n",
    "    if 'classification_report' in metrics and len(metrics['classification_report']) > 0:\n",
    "        # 假设是二分类，包含'0', '1', 'accuracy', 'macro avg', 'weighted avg'\n",
    "        report_keys = metrics['classification_report'][0].keys()\n",
    "        aggregated_report = {}\n",
    "        for key in report_keys:\n",
    "            if key == 'accuracy':\n",
    "                # 'accuracy' 是一个单一的浮点数\n",
    "                values = [m[key] for m in metrics['classification_report'] if key in m and isinstance(m[key], (int, float))]\n",
    "                if len(values) > 0:\n",
    "                    aggregated_report[key] = {\n",
    "                        'accuracy': np.mean(values)\n",
    "                    }\n",
    "            elif key in ['macro avg', 'weighted avg']:\n",
    "                # 这些是包含 'precision', 'recall', 'f1-score', 'support' 的字典\n",
    "                values = [m[key] for m in metrics['classification_report'] if key in m and isinstance(m[key], dict)]\n",
    "                if len(values) > 0:\n",
    "                    aggregated_report[key] = {\n",
    "                        'precision': np.mean([v['precision'] for v in values if 'precision' in v]),\n",
    "                        'recall': np.mean([v['recall'] for v in values if 'recall' in v]),\n",
    "                        'f1-score': np.mean([v['f1-score'] for v in values if 'f1-score' in v]),\n",
    "                        'support': int(np.mean([v['support'] for v in values if 'support' in v]))\n",
    "                    }\n",
    "            else:\n",
    "                # 每个类别的指标\n",
    "                values = [m[key] for m in metrics['classification_report'] if key in m and isinstance(m[key], dict)]\n",
    "                if len(values) > 0:\n",
    "                    aggregated_report[key] = {\n",
    "                        'precision': np.mean([v['precision'] for v in values if 'precision' in v]),\n",
    "                        'recall': np.mean([v['recall'] for v in values if 'recall' in v]),\n",
    "                        'f1-score': np.mean([v['f1-score'] for v in values if 'f1-score' in v]),\n",
    "                        'support': int(np.mean([v['support'] for v in values if 'support' in v]))\n",
    "                    }\n",
    "        averages['classification_report'] = aggregated_report\n",
    "    else:\n",
    "        averages['classification_report'] = None\n",
    "    \n",
    "    # 打印平均值和标准差\n",
    "    print(\"=== Metrics Averages ===\")\n",
    "    for metric in simple_metrics:\n",
    "        if averages[metric] is not None:\n",
    "            print(f\"{metric.capitalize()}: {averages[metric]:.4f} ± {stds[metric]:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric.capitalize()}: N/A\")\n",
    "    \n",
    "    if averages['confusion_matrix'] is not None:\n",
    "        print(\"\\nAverage Confusion Matrix:\")\n",
    "        print(np.array(averages['confusion_matrix']))\n",
    "    else:\n",
    "        print(\"\\nAverage Confusion Matrix: N/A\")\n",
    "    \n",
    "    if averages['classification_report'] is not None:\n",
    "        print(\"\\nAggregated Classification Report:\")\n",
    "        for key, value in averages['classification_report'].items():\n",
    "            print(f\"{key}:\")\n",
    "            for metric_name, metric_value in value.items():\n",
    "                if metric_value is not None:\n",
    "                    print(f\"  {metric_name}: {metric_value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"  {metric_name}: N/A\")\n",
    "    else:\n",
    "        print(\"\\nAggregated Classification Report: N/A\")\n",
    "    \n",
    "    return averages\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 调用average_metrics函数，传入JSON文件路径\n",
    "    metrics_averages = average_metrics('nested_cv_metrics.json')\n",
    "    \n",
    "    # 如果需要，可以进一步处理metrics_averages字典\n",
    "    # 例如，保存到另一个文件或进行可视化\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yzhongenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
