{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from load_feat_pd import load_feat  # Ensure this module is in your PYTHONPATH\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, base_folder_path: str, feature_names: List[str], log_value: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading and merging multiple features.\n",
    "\n",
    "        Args:\n",
    "            base_folder_path (str): Base path where feature folders are located.\n",
    "            feature_names (List[str]): List of feature names to load (e.g., ['energy', 'f0']).\n",
    "            log_value (bool): Whether to apply logarithmic transformation to feature values.\n",
    "        \"\"\"\n",
    "        base_folder_path_unnorm = base_folder_path.replace('_normalized', '')\n",
    "\n",
    "        self.base_folder_path = base_folder_path\n",
    "        self.feature_names = feature_names\n",
    "        self.log_value = log_value\n",
    "\n",
    "        # Load data for each feature\n",
    "        self.feature_data = {}\n",
    "        for feature in self.feature_names:\n",
    "            self.base_folder_path = base_folder_path_unnorm if feature == 'energy' else base_folder_path\n",
    "            print(f\"Loading feature: {feature}\")\n",
    "            data = load_feat(self.base_folder_path, feature_name=feature, log_value=self.log_value)\n",
    "            self.feature_data[feature] = data\n",
    "            print(f\"Loaded {len(data)} samples for feature '{feature}'\\n\")\n",
    "\n",
    "        # Merge data across features\n",
    "        self.merged_data = self._merge_features()\n",
    "\n",
    "    def _merge_features(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Merges data from different features based on the 'filename' key.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: A list of merged sample dictionaries containing all features.\n",
    "        \"\"\"\n",
    "        # Create a mapping from filename to data for each feature\n",
    "        feature_maps = {}\n",
    "        for feature_name, data in self.feature_data.items():\n",
    "            feature_map = {}\n",
    "            for sample in data:\n",
    "                filename = sample.get('filename')\n",
    "                if filename:\n",
    "                    feature_map[filename] = sample\n",
    "                else:\n",
    "                    print(f\"Warning: Sample without 'filename' in feature '{feature_name}'. Skipping.\")\n",
    "            feature_maps[feature_name] = feature_map\n",
    "            print(f\"Feature '{feature_name}' has {len(feature_map)} unique filenames.\")\n",
    "\n",
    "        # Find the intersection of filenames across all features\n",
    "        all_filenames = set.intersection(*(set(fm.keys()) for fm in feature_maps.values()))\n",
    "        print(f\"\\nTotal common filenames across all features: {len(all_filenames)}\\n\")\n",
    "\n",
    "        # Merge samples that are present in all feature sets\n",
    "        merged_samples = []\n",
    "        for filename in all_filenames:\n",
    "            merged_sample = {}\n",
    "            # Retrieve metadata from the first feature (assuming metadata is consistent across features)\n",
    "            first_feature = self.feature_names[0]\n",
    "            sample_data = feature_maps[first_feature][filename]\n",
    "\n",
    "            # Copy metadata fields\n",
    "            for key, value in sample_data.items():\n",
    "                if key != 'value' and key != 'filename':\n",
    "                    merged_sample[key] = value\n",
    "                elif key == 'filename':\n",
    "                    merged_sample[key] = value  # Keep 'filename'\n",
    "\n",
    "            # Add features with feature names as keys\n",
    "            for feature_name in self.feature_names:\n",
    "                feature_sample = feature_maps[feature_name][filename]\n",
    "                feature_value = feature_sample.get('value')\n",
    "                if feature_value is not None:\n",
    "                    merged_sample[feature_name] = feature_value\n",
    "                else:\n",
    "                    print(f\"Warning: 'value' missing for filename '{filename}' in feature '{feature_name}'.\")\n",
    "                    # Optionally handle missing feature values here\n",
    "\n",
    "            merged_samples.append(merged_sample)\n",
    "\n",
    "        print(f\"Total merged samples with all features: {len(merged_samples)}\")\n",
    "        return merged_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.merged_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing all features and metadata for the sample.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        return self.merged_data[idx]\n",
    "\n",
    "\n",
    "class SubsetFeatureDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, feature_names: List[str]):\n",
    "        \"\"\"\n",
    "        Initializes the subset dataset.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing the subset of data.\n",
    "            feature_names (List[str]): List of feature names included in the dataset.\n",
    "        \"\"\"\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.feature_names = feature_names\n",
    "        self.feats2level = {\n",
    "        'jitter': 'utt',\n",
    "        'shimmer': 'utt',\n",
    "        'rp': 'utt',\n",
    "        'f0': 'frame',\n",
    "        'energy': 'frame'\n",
    "        }\n",
    "        \n",
    "        self.label_mapping = {\n",
    "            '21': 0,\n",
    "            '22': 1\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\"\n",
    "        Retrieves the sample and its label at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, int]: A tuple containing concatenated features and the label.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        \n",
    "        # Initialize a list to hold processed feature values\n",
    "        processed_features = []\n",
    "        \n",
    "        # Iterate over each feature\n",
    "        for feature in self.feature_names:\n",
    "            level = self.feats2level.get(feature, 'utt')  # Default to 'utt' if not specified\n",
    "            feature_data = self.data.iloc[idx][feature]\n",
    "            \n",
    "            if level == 'frame':\n",
    "                # Ensure feature_data is a NumPy array for consistency\n",
    "                if isinstance(feature_data, list):\n",
    "                    feature_data = np.array(feature_data)\n",
    "                elif isinstance(feature_data, np.ndarray):\n",
    "                    pass\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported type for frame-level feature '{feature}': {type(feature_data)}\")\n",
    "                \n",
    "                # Calculate mean and standard deviation\n",
    "                mean = np.mean(feature_data)\n",
    "                std = np.std(feature_data)\n",
    "                \n",
    "                # Append mean and std to the processed_features list\n",
    "                processed_features.extend([mean, std])\n",
    "            else:\n",
    "                # For 'utt' level features\n",
    "                if isinstance(feature_data, np.ndarray):\n",
    "                    if feature_data.size == 1:\n",
    "                        # Single value, use it directly\n",
    "                        value = feature_data.item()\n",
    "                        processed_features.append(value)\n",
    "                    else:\n",
    "                        # Multiple values, flatten and extend\n",
    "                        processed_features.extend(feature_data.flatten().tolist())\n",
    "                else:\n",
    "                    # Assume it's a scalar\n",
    "                    processed_features.append(feature_data)\n",
    "        \n",
    "        # Convert the processed features list to a PyTorch tensor of type float32\n",
    "        concate_features = torch.tensor(processed_features, dtype=torch.float32)\n",
    "        \n",
    "        # Retrieve and map the label\n",
    "        group_id = self.data.iloc[idx]['group_id']\n",
    "        if group_id not in self.label_mapping:\n",
    "            raise ValueError(f\"Unexpected group_id '{group_id}'. Expected '21' or '22'.\")\n",
    "        label = self.label_mapping[group_id]\n",
    "        \n",
    "        return concate_features, label\n",
    "\n",
    "    def get_numpy(\n",
    "        self,\n",
    "        label_column: Optional[str] = None\n",
    "        ) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Converts the dataset to NumPy arrays for scikit-learn training.\n",
    "\n",
    "        Args:\n",
    "            label_column (str, optional): The name of the metadata column to use as labels.\n",
    "                                          If None, only features are returned.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, Optional[np.ndarray]]: Features and labels as NumPy arrays.\n",
    "                                                     If label_column is None, returns (X, None).\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        y = [] if label_column else None\n",
    "\n",
    "        for idx in range(len(self)):\n",
    "            features, label = self.__getitem__(idx)\n",
    "            X.append(features.numpy())\n",
    "            if label_column:\n",
    "                # Map the label using the predefined mapping\n",
    "                group_id = self.data.iloc[idx][label_column]\n",
    "                if group_id not in self.label_mapping:\n",
    "                    raise ValueError(f\"Unexpected group_id '{group_id}'. Expected '21' or '22'.\")\n",
    "                y.append(self.label_mapping[group_id])\n",
    "        \n",
    "        X = np.stack(X)  # Shape: (num_samples, num_features)\n",
    "        y = np.array(y) if label_column else None\n",
    "\n",
    "        return X, y\n",
    "\n",
    "def train_test_split_by_subject(\n",
    "    dataset: Dataset,\n",
    "    feature_names: List[str],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42\n",
    ") -> Tuple[SubsetFeatureDataset, SubsetFeatureDataset]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into train and test sets based on subject_id, ensuring no overlapping subjects\n",
    "    and maintaining gender balance in the test set.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The merged dataset (FeatureDataset instance).\n",
    "        feature_names (List[str]): List of feature names included in the dataset.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[SubsetFeatureDataset, SubsetFeatureDataset]: Train and test datasets.\n",
    "    \"\"\"\n",
    "    # Convert merged data to DataFrame\n",
    "    merged_data = dataset.merged_data  # Assuming 'merged_data' is a list of dicts\n",
    "    df = pd.DataFrame(merged_data)\n",
    "    \n",
    "    # Extract unique subject_ids and their genders\n",
    "    subjects_df = df[['subject_id', 'gender']].drop_duplicates()\n",
    "\n",
    "    # Check for missing values in 'gender'\n",
    "    subjects_df = subjects_df.dropna(subset=['gender'])\n",
    "\n",
    "    # Ensure 'gender' is categorical\n",
    "    subjects_df['gender'] = subjects_df['gender'].astype(str)\n",
    "\n",
    "    # Perform stratified split based on gender\n",
    "    train_subjects, test_subjects = train_test_split(\n",
    "        subjects_df,\n",
    "        test_size=test_size,\n",
    "        stratify=subjects_df['gender'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Extract lists of subject_ids\n",
    "    train_subject_ids = set(train_subjects['subject_id'])\n",
    "    test_subject_ids = set(test_subjects['subject_id'])\n",
    "\n",
    "    print(f\"Total subjects: {len(subjects_df)}\")\n",
    "    print(f\"Training subjects: {len(train_subject_ids)}\")\n",
    "    print(f\"Testing subjects: {len(test_subject_ids)}\")\n",
    "    \n",
    "  \n",
    "    \n",
    "\n",
    "    # Assign samples to train and test sets based on subject_id\n",
    "    train_df = df[df['subject_id'].isin(train_subject_ids)].reset_index(drop=True)\n",
    "    test_df = df[df['subject_id'].isin(test_subject_ids)].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Total training samples: {len(train_df)}\")\n",
    "    print(f\"Total testing samples: {len(test_df)}\")\n",
    "    \n",
    "\n",
    "    # Verify gender balance in test set\n",
    "    test_gender_counts = test_df['gender'].value_counts(normalize=True)\n",
    "    print(\"\\nGender distribution in test set:\")\n",
    "    print(test_gender_counts)\n",
    "    \n",
    "    # Verify group balance in test set\n",
    "    test_group_counts = test_df['group_id'].value_counts(normalize=True)\n",
    "    print(\"\\nGroup distribution in test set:\")\n",
    "    print(test_group_counts)\n",
    "    \n",
    "\n",
    "    # Create subset datasets\n",
    "    train_dataset = SubsetFeatureDataset(train_df, feature_names)\n",
    "    test_dataset = SubsetFeatureDataset(test_df, feature_names)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading feature: jitter\n",
      "Processing PictureNaming folder...\n",
      "Found 1652 npy files\n",
      "0 files with all 0 values\n",
      "Processing EarlyLate folder...\n",
      "Found 3971 npy files\n",
      "nan in /data/storage025/Turntaking/wavs_single_channel_normalized_nosil/EarlyLate-features/jitter/subj-2120_27_E_kwaad_geloof.wav_1.wav_jitter.npy\n",
      "0 files with all 0 values\n",
      "Processing BoundaryTone folder...\n",
      "Found 5026 npy files\n",
      "0 files with all 0 values\n",
      "Loaded 7918 samples for feature 'jitter'\n",
      "\n",
      "Loading feature: shimmer\n",
      "Processing PictureNaming folder...\n",
      "Found 1652 npy files\n",
      "0 files with all 0 values\n",
      "Processing EarlyLate folder...\n",
      "Found 3971 npy files\n",
      "nan in /data/storage025/Turntaking/wavs_single_channel_normalized_nosil/EarlyLate-features/shimmer/subj-2120_27_E_kwaad_geloof.wav_1.wav_shimmer.npy\n",
      "0 files with all 0 values\n",
      "Processing BoundaryTone folder...\n",
      "Found 5026 npy files\n",
      "0 files with all 0 values\n",
      "Loaded 7918 samples for feature 'shimmer'\n",
      "\n",
      "Loading feature: rp\n",
      "Processing PictureNaming folder...\n",
      "Found 1652 npy files\n",
      "0 files with all 0 values\n",
      "Processing EarlyLate folder...\n",
      "Found 3971 npy files\n",
      "0 files with all 0 values\n",
      "Processing BoundaryTone folder...\n",
      "Found 5026 npy files\n",
      "0 files with all 0 values\n",
      "Loaded 7919 samples for feature 'rp'\n",
      "\n",
      "Loading feature: f0\n",
      "Processing PictureNaming folder...\n",
      "Found 1652 npy files\n",
      "All value larger than 500.0 in /data/storage025/Turntaking/wavs_single_channel_normalized_nosil/PictureNaming-features/f0/subj-2112_rups.png_1.wav_f0.npy\n",
      "All value larger than 500.0 in /data/storage025/Turntaking/wavs_single_channel_normalized_nosil/PictureNaming-features/f0/subj-2202_schatkist.png_1.wav_f0.npy\n",
      "All value larger than 500.0 in /data/storage025/Turntaking/wavs_single_channel_normalized_nosil/PictureNaming-features/f0/subj-2127_rups.png_1.wav_f0.npy\n",
      "All value larger than 500.0 in /data/storage025/Turntaking/wavs_single_channel_normalized_nosil/PictureNaming-features/f0/subj-2120_kaars.png_1.wav_f0.npy\n",
      "0 files with all 0 values\n",
      "Processing EarlyLate folder...\n",
      "Found 3971 npy files\n",
      "All value larger than 500.0 in /data/storage025/Turntaking/wavs_single_channel_normalized_nosil/EarlyLate-features/f0/subj-2112_41_L_gereedschap_tanden.wav_1.wav_f0.npy\n",
      "All value larger than 500.0 in /data/storage025/Turntaking/wavs_single_channel_normalized_nosil/EarlyLate-features/f0/subj-2120_17_E_piept_knaagdier.wav_1.wav_f0.npy\n",
      "0 files with all 0 values\n",
      "Processing BoundaryTone folder...\n",
      "Found 5026 npy files\n",
      "0 files with all 0 values\n",
      "Loaded 7913 samples for feature 'f0'\n",
      "\n",
      "Loading feature: energy\n",
      "Processing PictureNaming folder...\n",
      "Found 1652 npy files\n",
      "0 files with all 0 values\n",
      "Processing EarlyLate folder...\n",
      "Found 3971 npy files\n",
      "0 files with all 0 values\n",
      "Processing BoundaryTone folder...\n",
      "Found 5026 npy files\n",
      "0 files with all 0 values\n",
      "Loaded 7919 samples for feature 'energy'\n",
      "\n",
      "Feature 'jitter' has 7817 unique filenames.\n",
      "Feature 'shimmer' has 7817 unique filenames.\n",
      "Feature 'rp' has 7818 unique filenames.\n",
      "Feature 'f0' has 7812 unique filenames.\n",
      "Feature 'energy' has 7818 unique filenames.\n",
      "\n",
      "Total common filenames across all features: 7811\n",
      "\n",
      "Total merged samples with all features: 7811\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    feats2level = {\n",
    "        'jitter': 'utt',\n",
    "        'shimmer': 'utt',\n",
    "        'rp': 'utt',\n",
    "        'f0': 'frame',\n",
    "        'energy': 'frame'\n",
    "    }\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "    features_to_load = ['jitter', 'shimmer', 'rp', 'f0', 'energy']\n",
    "    # features_to_load = ['energy', 'rp']\n",
    "    # for feat in allfeats:\n",
    "    \n",
    "    base_folder = '/data/storage025/Turntaking/wavs_single_channel_normalized_nosil'\n",
    "\n",
    "    # Initialize the merged dataset\n",
    "    merged_dataset = FeatureDataset(\n",
    "        base_folder_path=base_folder,\n",
    "        feature_names=features_to_load,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subjects: 52\n",
      "Training subjects: 41\n",
      "Testing subjects: 11\n",
      "Total training samples: 6143\n",
      "Total testing samples: 1668\n",
      "\n",
      "Gender distribution in test set:\n",
      "gender\n",
      "V    0.551559\n",
      "M    0.448441\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Group distribution in test set:\n",
      "group_id\n",
      "21    0.726019\n",
      "22    0.273981\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Training Dataset: 6143 samples\n",
      "Testing Dataset: 1668 samples\n",
      "[tensor([[4.6999e-02, 1.0653e-01, 3.4400e-01, 1.4356e+02, 1.3923e+01, 5.2841e-04,\n",
      "         7.3976e-04],\n",
      "        [2.2267e-02, 8.6426e-02, 9.0167e-01, 1.0380e+02, 1.7365e+01, 4.6534e-03,\n",
      "         4.1072e-03],\n",
      "        [3.0747e-02, 1.0213e-01, 7.9200e-01, 1.6771e+02, 1.6348e+01, 1.7342e-01,\n",
      "         1.7932e-01],\n",
      "        [2.2707e-02, 7.5624e-02, 1.0800e+00, 2.2212e+02, 3.1100e+01, 4.3099e-01,\n",
      "         4.7440e-01],\n",
      "        [4.0926e-02, 1.2065e-01, 7.9200e-01, 1.0259e+02, 1.3008e+01, 7.9003e-01,\n",
      "         8.4758e-01],\n",
      "        [6.5835e-02, 2.2473e-01, 9.9566e-01, 1.2616e+02, 2.4706e+01, 1.0136e-01,\n",
      "         1.3000e-01],\n",
      "        [3.1875e-02, 9.0092e-02, 8.2400e-01, 1.8305e+02, 4.0010e+01, 5.6903e-02,\n",
      "         1.1415e-01],\n",
      "        [2.2051e-02, 1.1951e-01, 2.3836e-01, 1.0174e+02, 1.2080e+01, 9.9862e-02,\n",
      "         8.2660e-02],\n",
      "        [3.1314e-02, 7.9229e-02, 1.6240e+00, 1.2833e+02, 2.0424e+01, 2.3701e-01,\n",
      "         3.1237e-01],\n",
      "        [3.8714e-02, 1.6543e-01, 1.3999e+00, 1.2884e+02, 1.4307e+01, 1.5244e-03,\n",
      "         1.9327e-03],\n",
      "        [2.4945e-02, 1.2375e-01, 8.2400e-01, 1.1623e+02, 1.8276e+01, 6.4387e-01,\n",
      "         9.1893e-01],\n",
      "        [2.9887e-02, 1.1753e-01, 2.9188e+00, 1.6854e+02, 1.9699e+01, 4.3815e-01,\n",
      "         6.9135e-01],\n",
      "        [2.2917e-02, 1.0161e-01, 6.2818e-01, 1.7249e+02, 1.5984e+01, 4.7405e-01,\n",
      "         6.0734e-01],\n",
      "        [3.1053e-02, 1.3248e-01, 6.3200e-01, 9.6807e+01, 7.6490e+00, 8.4522e-04,\n",
      "         9.5109e-04],\n",
      "        [2.4582e-02, 1.1352e-01, 1.6240e+00, 1.0504e+02, 1.7156e+01, 3.1320e-01,\n",
      "         5.1338e-01],\n",
      "        [3.5856e-02, 1.7836e-01, 3.5629e-01, 1.6678e+02, 6.8153e+00, 1.1714e-01,\n",
      "         1.4282e-01]]), tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0])]\n",
      "2\n",
      "Train Batch 1:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Perform train-test split\n",
    "    train_dataset, test_dataset = train_test_split_by_subject(\n",
    "        merged_dataset,\n",
    "        feature_names=features_to_load,\n",
    "        test_size=0.2,        # 20% for testing\n",
    "        random_state=523       # For reproducibility\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining Dataset: {len(train_dataset)} samples\")\n",
    "    print(f\"Testing Dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,      # Adjust based on your memory constraints\n",
    "        shuffle=True,       # Shuffle for training\n",
    "        num_workers=4       # Adjust based on your CPU cores\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=16,      # Adjust as needed\n",
    "        shuffle=False,      # No need to shuffle for testing\n",
    "        num_workers=4       # Adjust based on your CPU cores\n",
    "    )\n",
    "\n",
    "    # Example: Iterate over the training DataLoader\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Access features and metadata\n",
    "        print(batch)\n",
    "        print(len(batch))\n",
    "        # ... and so on for other metadata and features\n",
    "\n",
    "        # Example: Print batch information\n",
    "        print(f\"Train Batch {batch_idx + 1}:\")\n",
    " \n",
    "\n",
    "        # Break after the first batch for demonstration\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of X_train: (6143, 7)\n",
      "Shape of y_train: (6143,)\n",
      "Shape of X_test: (1668, 7)\n",
      "Shape of y_test: (1668,)\n",
      "\n",
      "First 5 training labels: [0 0 1 1 1]\n",
      "First 5 testing labels: [0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Get NumPy arrays for scikit-learn (classification based on 'group_id')\n",
    "X_train, y_train = train_dataset.get_numpy(label_column='group_id')\n",
    "X_test, y_test = test_dataset.get_numpy(label_column='group_id')\n",
    "\n",
    "print(\"\\nShape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "\n",
    "print(\"\\nFirst 5 training labels:\", y_train[:5])\n",
    "print(\"First 5 testing labels:\", y_test[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yzhongenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
