{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Dict\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from load_feat_pd import load_feat  # Ensure this module is in your PYTHONPATH\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, base_folder_path: str, feature_names: List[str], log_value: bool = False):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading and merging multiple features.\n",
    "\n",
    "        Args:\n",
    "            base_folder_path (str): Base path where feature folders are located.\n",
    "            feature_names (List[str]): List of feature names to load (e.g., ['energy', 'f0']).\n",
    "            log_value (bool): Whether to apply logarithmic transformation to feature values.\n",
    "        \"\"\"\n",
    "        base_folder_path_unnorm = base_folder_path.replace('_normalized', '')\n",
    "\n",
    "        self.base_folder_path = base_folder_path\n",
    "        self.feature_names = feature_names\n",
    "        self.log_value = log_value\n",
    "\n",
    "        # Load data for each feature\n",
    "        self.feature_data = {}\n",
    "        for feature in self.feature_names:\n",
    "            self.base_folder_path = base_folder_path_unnorm if feature == 'energy' else base_folder_path\n",
    "            print(f\"Loading feature: {feature}\")\n",
    "            data = load_feat(self.base_folder_path, feature_name=feature, log_value=self.log_value)\n",
    "            self.feature_data[feature] = data\n",
    "            print(f\"Loaded {len(data)} samples for feature '{feature}'\\n\")\n",
    "\n",
    "        # Merge data across features\n",
    "        self.merged_data = self._merge_features()\n",
    "\n",
    "    def _merge_features(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Merges data from different features based on the 'filename' key.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: A list of merged sample dictionaries containing all features.\n",
    "        \"\"\"\n",
    "        # Create a mapping from filename to data for each feature\n",
    "        feature_maps = {}\n",
    "        for feature_name, data in self.feature_data.items():\n",
    "            feature_map = {}\n",
    "            for sample in data:\n",
    "                filename = sample.get('filename')\n",
    "                if filename:\n",
    "                    feature_map[filename] = sample\n",
    "                else:\n",
    "                    print(f\"Warning: Sample without 'filename' in feature '{feature_name}'. Skipping.\")\n",
    "            feature_maps[feature_name] = feature_map\n",
    "            print(f\"Feature '{feature_name}' has {len(feature_map)} unique filenames.\")\n",
    "\n",
    "        # Find the intersection of filenames across all features\n",
    "        all_filenames = set.intersection(*(set(fm.keys()) for fm in feature_maps.values()))\n",
    "        print(f\"\\nTotal common filenames across all features: {len(all_filenames)}\\n\")\n",
    "\n",
    "        # Merge samples that are present in all feature sets\n",
    "        merged_samples = []\n",
    "        for filename in all_filenames:\n",
    "            merged_sample = {}\n",
    "            # Retrieve metadata from the first feature (assuming metadata is consistent across features)\n",
    "            first_feature = self.feature_names[0]\n",
    "            sample_data = feature_maps[first_feature][filename]\n",
    "\n",
    "            # Copy metadata fields\n",
    "            for key, value in sample_data.items():\n",
    "                if key != 'value' and key != 'filename':\n",
    "                    merged_sample[key] = value\n",
    "                elif key == 'filename':\n",
    "                    merged_sample[key] = value  # Keep 'filename'\n",
    "\n",
    "            # Add features with feature names as keys\n",
    "            for feature_name in self.feature_names:\n",
    "                feature_sample = feature_maps[feature_name][filename]\n",
    "                feature_value = feature_sample.get('value')\n",
    "                if feature_value is not None:\n",
    "                    merged_sample[feature_name] = feature_value\n",
    "                else:\n",
    "                    print(f\"Warning: 'value' missing for filename '{filename}' in feature '{feature_name}'.\")\n",
    "                    # Optionally handle missing feature values here\n",
    "\n",
    "            merged_samples.append(merged_sample)\n",
    "\n",
    "        print(f\"Total merged samples with all features: {len(merged_samples)}\")\n",
    "        return merged_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.merged_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing all features and metadata for the sample.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        return self.merged_data[idx]\n",
    "\n",
    "\n",
    "class SubsetFeatureDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, feature_names: List[str]):\n",
    "        \"\"\"\n",
    "        Initializes the subset dataset.\n",
    "\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): DataFrame containing the subset of data.\n",
    "            feature_names (List[str]): List of feature names included in the dataset.\n",
    "        \"\"\"\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.feature_names = feature_names\n",
    "        self.feats2level = {\n",
    "        'jitter': 'utt',\n",
    "        'shimmer': 'utt',\n",
    "        'rp': 'utt',\n",
    "        'f0': 'frame',\n",
    "        'energy': 'frame'\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the sample at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing all processed features for the sample.\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= len(self):\n",
    "            raise IndexError(\"Index out of range\")\n",
    "        \n",
    "        # Initialize a list to hold processed feature values\n",
    "        processed_features = []\n",
    "        \n",
    "        # Iterate over each feature\n",
    "        for feature in self.feature_names:\n",
    "            level = self.feats2level.get(feature, 'utt')  # Default to 'utt' if not specified\n",
    "            feature_data = self.data.iloc[idx][feature]\n",
    "            \n",
    "            if level == 'frame':\n",
    "                # Ensure feature_data is a NumPy array for consistency\n",
    "                if isinstance(feature_data, list):\n",
    "                    feature_data = np.array(feature_data)\n",
    "                \n",
    "                # Calculate mean and standard deviation\n",
    "                mean = np.mean(feature_data)\n",
    "                std = np.std(feature_data)\n",
    "                \n",
    "                # Append mean and std to the processed_features list\n",
    "                processed_features.extend([mean, std])\n",
    "            else:\n",
    "                # For 'utt' level features\n",
    "                if isinstance(feature_data, np.ndarray):\n",
    "                    # If the feature is an array, flatten it and extend the list\n",
    "                    processed_features.extend(feature_data.flatten())\n",
    "                else:\n",
    "                    # If the feature is a scalar, append it directly\n",
    "                    processed_features.append(feature_data)\n",
    "        \n",
    "        # Convert the processed features list to a PyTorch tensor of type float32\n",
    "        concate_features = torch.tensor(processed_features, dtype=torch.float32)\n",
    "        \n",
    "        return concate_features\n",
    "    \n",
    "\n",
    "def train_test_split_by_subject(\n",
    "    dataset: Dataset,\n",
    "    feature_names: List[str],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42\n",
    ") -> Tuple[SubsetFeatureDataset, SubsetFeatureDataset]:\n",
    "    \"\"\"\n",
    "    Splits the dataset into train and test sets based on subject_id, ensuring no overlapping subjects\n",
    "    and maintaining gender balance in the test set.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The merged dataset (FeatureDataset instance).\n",
    "        feature_names (List[str]): List of feature names included in the dataset.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[SubsetFeatureDataset, SubsetFeatureDataset]: Train and test datasets.\n",
    "    \"\"\"\n",
    "    # Convert merged data to DataFrame\n",
    "    merged_data = dataset.merged_data  # Assuming 'merged_data' is a list of dicts\n",
    "    df = pd.DataFrame(merged_data)\n",
    "    \n",
    "    # Extract unique subject_ids and their genders\n",
    "    subjects_df = df[['subject_id', 'gender']].drop_duplicates()\n",
    "\n",
    "    # Check for missing values in 'gender'\n",
    "    subjects_df = subjects_df.dropna(subset=['gender'])\n",
    "\n",
    "    # Ensure 'gender' is categorical\n",
    "    subjects_df['gender'] = subjects_df['gender'].astype(str)\n",
    "\n",
    "    # Perform stratified split based on gender\n",
    "    train_subjects, test_subjects = train_test_split(\n",
    "        subjects_df,\n",
    "        test_size=test_size,\n",
    "        stratify=subjects_df['gender'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Extract lists of subject_ids\n",
    "    train_subject_ids = set(train_subjects['subject_id'])\n",
    "    test_subject_ids = set(test_subjects['subject_id'])\n",
    "\n",
    "    print(f\"Total subjects: {len(subjects_df)}\")\n",
    "    print(f\"Training subjects: {len(train_subject_ids)}\")\n",
    "    print(f\"Testing subjects: {len(test_subject_ids)}\")\n",
    "    \n",
    "  \n",
    "    \n",
    "\n",
    "    # Assign samples to train and test sets based on subject_id\n",
    "    train_df = df[df['subject_id'].isin(train_subject_ids)].reset_index(drop=True)\n",
    "    test_df = df[df['subject_id'].isin(test_subject_ids)].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Total training samples: {len(train_df)}\")\n",
    "    print(f\"Total testing samples: {len(test_df)}\")\n",
    "    \n",
    "\n",
    "    # Verify gender balance in test set\n",
    "    test_gender_counts = test_df['gender'].value_counts(normalize=True)\n",
    "    print(\"\\nGender distribution in test set:\")\n",
    "    print(test_gender_counts)\n",
    "    \n",
    "    # Verify group balance in test set\n",
    "    test_group_counts = test_df['group_id'].value_counts(normalize=True)\n",
    "    print(\"\\nGroup distribution in test set:\")\n",
    "    print(test_group_counts)\n",
    "    \n",
    "\n",
    "    # Create subset datasets\n",
    "    train_dataset = SubsetFeatureDataset(train_df, feature_names)\n",
    "    test_dataset = SubsetFeatureDataset(test_df, feature_names)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    feats2level = {\n",
    "        'jitter': 'utt',\n",
    "        'shimmer': 'utt',\n",
    "        'rp': 'utt',\n",
    "        'f0': 'frame',\n",
    "        'energy': 'frame'\n",
    "    }\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "    features_to_load = ['jitter', 'shimmer', 'rp', 'f0', 'energy']\n",
    "    # features_to_load = ['energy', 'rp']\n",
    "    # for feat in allfeats:\n",
    "    \n",
    "    base_folder = '/data/storage025/Turntaking/wavs_single_channel_normalized_nosil'\n",
    "\n",
    "    # Initialize the merged dataset\n",
    "    merged_dataset = FeatureDataset(\n",
    "        base_folder_path=base_folder,\n",
    "        feature_names=features_to_load,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Perform train-test split\n",
    "    train_dataset, test_dataset = train_test_split_by_subject(\n",
    "        merged_dataset,\n",
    "        feature_names=features_to_load,\n",
    "        test_size=0.2,        # 20% for testing\n",
    "        random_state=523       # For reproducibility\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining Dataset: {len(train_dataset)} samples\")\n",
    "    print(f\"Testing Dataset: {len(test_dataset)} samples\")\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,      # Adjust based on your memory constraints\n",
    "        shuffle=True,       # Shuffle for training\n",
    "        num_workers=4       # Adjust based on your CPU cores\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=16,      # Adjust as needed\n",
    "        shuffle=False,      # No need to shuffle for testing\n",
    "        num_workers=4       # Adjust based on your CPU cores\n",
    "    )\n",
    "\n",
    "    # Example: Iterate over the training DataLoader\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        # Access features and metadata\n",
    "        print(batch)\n",
    "        print(len(batch))\n",
    "        # ... and so on for other metadata and features\n",
    "\n",
    "        # Example: Print batch information\n",
    "        print(f\"Train Batch {batch_idx + 1}:\")\n",
    " \n",
    "\n",
    "        # Break after the first batch for demonstration\n",
    "        break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yzhongenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
