{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Dict\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "class MultiFeatureDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 feature_dirs: Dict[str, str],\n",
    "                 feature_types: List[str],\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_dirs (Dict[str, str]): Dictionary mapping feature set names to their directories.\n",
    "                                           e.g., {\n",
    "                                               'BoundaryTone': 'path/to/BoundaryTone-features',\n",
    "                                               'EarlyLate': 'path/to/EarlyLate-features',\n",
    "                                               'PictureNaming': 'path/to/PictureNaming-features'\n",
    "                                           }\n",
    "            feature_types (List[str]): List of feature types (subfolder names) to include.\n",
    "                                       e.g., ['energy', 'f0', 'f0-4096', 'jitter', 'rp', 'shimmer']\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.feature_dirs = feature_dirs\n",
    "        self.feature_types = feature_types\n",
    "        self.transform = transform\n",
    "        self.samples = self._gather_samples()\n",
    "\n",
    "    def _gather_samples(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Gather all samples ensuring that each sample has corresponding features in all directories and feature types.\n",
    "        Returns a list of dictionaries with feature set names and feature types as keys and file paths as values.\n",
    "        Also extracts the subject ID.\n",
    "        \"\"\"\n",
    "        # Mapping: feature_set -> feature_type -> sample_id -> file_path\n",
    "        feature_files = defaultdict(lambda: defaultdict(dict))\n",
    "        sample_id_pattern = re.compile(r'^(subj-\\d+_.+\\.wav)_(\\w+)\\.npy$')\n",
    "\n",
    "        \n",
    "        for feature_set, dir_path in self.feature_dirs.items():\n",
    "            print(f\"Processing feature set: {feature_set}\")\n",
    "            for feature_type in self.feature_types:\n",
    "                feature_type_dir = Path(dir_path) / feature_type\n",
    "                if not feature_type_dir.is_dir():\n",
    "                    print(f\"  Warning: Feature type directory not found: {feature_type_dir}\")\n",
    "                    continue\n",
    "                files = list(feature_type_dir.glob('*.npy'))\n",
    "                print(f\"  Feature type '{feature_type}' has {len(files)} files.\")\n",
    "                for file_path in files:\n",
    "                    filename = file_path.name\n",
    "                    match = sample_id_pattern.match(filename)\n",
    "                    if match:\n",
    "                        sample_id, feature_suffix = match.groups()\n",
    "                        # Verify that the feature_suffix matches the current feature_type\n",
    "                        if feature_suffix != feature_type:\n",
    "                            print(f\"    Warning: Feature suffix '{feature_suffix}' does not match feature type '{feature_type}' in file '{filename}'. Skipping.\")\n",
    "                            continue\n",
    "                        feature_files[feature_set][feature_type][sample_id] = str(file_path)\n",
    "                    else:\n",
    "                        print(f\"    Warning: Filename does not match pattern and will be skipped: {filename}\")\n",
    "        \n",
    "        # Now, find common sample_ids across all feature sets and feature types\n",
    "        print(\"\\nGathering common samples across all feature sets and feature types...\")\n",
    "        sample_ids_per_feature_set = defaultdict(set)\n",
    "        for feature_set in self.feature_dirs.keys():\n",
    "            for feature_type in self.feature_types:\n",
    "                sample_ids = set(feature_files[feature_set][feature_type].keys())\n",
    "                sample_ids_per_feature_set[feature_set].add(feature_type)\n",
    "        \n",
    "        # Collect sample_ids that have all feature sets and all feature types\n",
    "        all_sample_ids = None\n",
    "        for feature_set in self.feature_dirs.keys():\n",
    "            for feature_type in self.feature_types:\n",
    "                current_ids = set(feature_files[feature_set][feature_type].keys())\n",
    "                if all_sample_ids is None:\n",
    "                    all_sample_ids = current_ids\n",
    "                else:\n",
    "                    all_sample_ids = all_sample_ids.intersection(current_ids)\n",
    "        \n",
    "        if not all_sample_ids:\n",
    "            print(\"No common samples found across all feature sets and feature types.\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Total common samples found: {len(all_sample_ids)}\\n\")\n",
    "        \n",
    "        # Now, create sample entries\n",
    "        samples = []\n",
    "        for sample_id in all_sample_ids:\n",
    "            sample_entry = {}\n",
    "            # Extract subject_id from sample_id\n",
    "            subject_id_match = re.match(r'subj-(\\d+)_', sample_id)\n",
    "            if subject_id_match:\n",
    "                subject_id = subject_id_match.group(1)\n",
    "                sample_entry['subject_id'] = subject_id\n",
    "            else:\n",
    "                print(f\"    Warning: Could not extract subject_id from sample_id '{sample_id}'. Skipping.\")\n",
    "                continue\n",
    "            # Collect all feature file paths\n",
    "            missing_feature = False\n",
    "            for feature_set in self.feature_dirs.keys():\n",
    "                for feature_type in self.feature_types:\n",
    "                    file_path = feature_files[feature_set][feature_type].get(sample_id)\n",
    "                    if file_path:\n",
    "                        key = f\"{feature_set}_{feature_type}\"\n",
    "                        sample_entry[key] = file_path\n",
    "                    else:\n",
    "                        print(f\"    Warning: Missing file for sample_id '{sample_id}' in feature_set '{feature_set}', feature_type '{feature_type}'. Skipping sample.\")\n",
    "                        missing_feature = True\n",
    "                        break\n",
    "                if missing_feature:\n",
    "                    break\n",
    "            if not missing_feature:\n",
    "                samples.append(sample_entry)\n",
    "        \n",
    "        print(f\"Total valid samples after checking all feature sets and feature types: {len(samples)}\")\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        sample_info = self.samples[idx]\n",
    "        features = []\n",
    "        for key in sorted(sample_info.keys()):\n",
    "            if key == 'subject_id':\n",
    "                continue\n",
    "            feature = np.load(sample_info[key])\n",
    "                 # print all the shapes of the features\n",
    "            if feature.shape == () :\n",
    "                feature = np.array([feature])\n",
    "            elif feature.shape != (1,) :\n",
    "                feature = np.array([np.mean(feature, axis=0), np.std(feature, axis=0), np.median(feature, axis=0)])\n",
    "            \n",
    "            features.append(feature)\n",
    "        \n",
    "        # Concatenate all features into a single feature vector\n",
    "        # Assuming all features are 1D arrays. Adjust if features have different dimensions.\n",
    "        try:\n",
    "            combined_features = np.concatenate(features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating features for sample index {idx}: {e}\")\n",
    "       \n",
    "            combined_features = np.array([])\n",
    "        \n",
    "        subject_id = sample_info['subject_id']\n",
    "\n",
    "        if self.transform:\n",
    "            combined_features = self.transform(combined_features)\n",
    "        else:\n",
    "            combined_features = torch.tensor(combined_features, dtype=torch.float32)\n",
    "\n",
    "        return combined_features, subject_id\n",
    "\n",
    "    def get_numpy_data(self) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"\n",
    "        Returns all data as a NumPy array and a list of subject IDs.\n",
    "        Suitable for scikit-learn.\n",
    "        \"\"\"\n",
    "        all_features = []\n",
    "        subject_ids = []\n",
    "        for idx, sample in enumerate(self.samples):\n",
    "            features = []\n",
    "            for key in sorted(sample.keys()):\n",
    "                if key == 'subject_id':\n",
    "                    continue\n",
    "                feature = np.load(sample[key])\n",
    "                \n",
    "                if feature.shape == () :\n",
    "                    feature = np.array([feature])\n",
    "                elif feature.shape != (1,) :\n",
    "                    feature = np.array([np.mean(feature, axis=0), np.std(feature, axis=0), np.median(feature, axis=0)])\n",
    "            \n",
    "                features.append(feature)\n",
    "            try:\n",
    "                combined_features = np.concatenate(features)\n",
    "            except Exception as e:\n",
    "                print(f\"Error concatenating features for sample index {idx}: {e}\")\n",
    "                continue\n",
    "            all_features.append(combined_features)\n",
    "            subject_ids.append(sample['subject_id'])\n",
    "        return np.array(all_features), subject_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing feature set: test\n",
      "  Feature type 'energy' has 31 files.\n",
      "  Feature type 'f0' has 31 files.\n",
      "  Feature type 'jitter' has 31 files.\n",
      "  Feature type 'rp' has 31 files.\n",
      "\n",
      "Gathering common samples across all feature sets and feature types...\n",
      "Total common samples found: 31\n",
      "\n",
      "Total valid samples after checking all feature sets and feature types: 31\n",
      "\n",
      "Total samples in dataset: 31\n",
      "\n",
      "Batch 1:\n",
      "  Features shape: torch.Size([2, 8])\n",
      "  Subject IDs: ('2112', '2112')\n",
      "\n",
      "Feature matrix shape: (31, 8)\n",
      "Subject IDs (前5个): ['2112', '2112', '2112', '2112', '2112']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义特征目录路径\n",
    "    basedir = '/data/storage025/Turntaking/wavs_single_channel_normalized_nosil/'\n",
    "\n",
    "    # feature_dirs = {'test'  : os.path.join(basedir, 'test-features')}\n",
    "    feature_dirs = {'PictureNaming'  : os.path.join(basedir, 'PictureNaming-features'),}\n",
    "\n",
    "\n",
    "    # 定义特征类型（子文件夹名称）\n",
    "    feature_types = ['energy', 'f0', 'jitter', 'rp']\n",
    "\n",
    "    # 初始化数据集\n",
    "    dataset = MultiFeatureDataset(feature_dirs=feature_dirs, feature_types=feature_types)\n",
    "\n",
    "    # 检查样本数量\n",
    "    print(f\"\\nTotal samples in dataset: {len(dataset)}\\n\")\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"No samples available for DataLoader. Please check your directory structure and file naming conventions.\")\n",
    "    else:\n",
    "        # 创建 DataLoader\n",
    "        torch_dataloader = DataLoader(dataset, batch_size=2, shuffle=True, num_workers=4)\n",
    "\n",
    "        # 示例遍历 DataLoader\n",
    "        for batch_idx, (batch_features, batch_subject_ids) in enumerate(torch_dataloader):\n",
    "            print(f\"Batch {batch_idx + 1}:\")\n",
    "            print(f\"  Features shape: {batch_features.shape}\")\n",
    "            print(f\"  Subject IDs: {batch_subject_ids}\")\n",
    "            # 在此处添加您的训练代码\n",
    "            break  # 仅示例一次迭代\n",
    "        \n",
    "    X, subject_ids = dataset.get_numpy_data()\n",
    "    print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "    print(f\"Subject IDs (前5个): {subject_ids[:5]}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用示例\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义特征目录路径\n",
    "    feature_dirs = {\n",
    "        'BoundaryTone': '/path/to/BoundaryTone-features',\n",
    "        'EarlyLate': '/path/to/EarlyLate-features',\n",
    "        'PictureNaming': '/path/to/PictureNaming-features'\n",
    "    }\n",
    "\n",
    "    # 定义特征类型（子文件夹名称）\n",
    "    feature_types = ['energy', 'f0', 'f0-4096', 'jitter', 'rp', 'shimmer']\n",
    "\n",
    "    # 初始化数据集\n",
    "    dataset = MultiFeatureDataset(feature_dirs=feature_dirs, feature_types=feature_types)\n",
    "\n",
    "    # 检查样本数量\n",
    "    print(f\"\\nTotal samples in dataset: {len(dataset)}\\n\")\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"No samples available for DataLoader. Please check your directory structure and file naming conventions.\")\n",
    "    else:\n",
    "        # 创建 DataLoader\n",
    "        torch_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "        # 示例遍历 DataLoader\n",
    "        for batch_idx, (batch_features, batch_subject_ids) in enumerate(torch_dataloader):\n",
    "            print(f\"Batch {batch_idx + 1}:\")\n",
    "            print(f\"  Features shape: {batch_features.shape}\")\n",
    "            print(f\"  Subject IDs: {batch_subject_ids}\")\n",
    "            # 在此处添加您的训练代码\n",
    "            break  # 仅示例一次迭代\n",
    "\n",
    "        # 对于 scikit-learn：获取 NumPy 数组\n",
    "        X, subject_ids = dataset.get_numpy_data()\n",
    "        print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "        print(f\"Subject IDs (前5个): {subject_ids[:5]}\")  # 打印前5个subject_id\n",
    "\n",
    "        # 示例 scikit-learn 使用\n",
    "        # from sklearn.model_selection import train_test_split\n",
    "        # from sklearn.ensemble import RandomForestClassifier\n",
    "        # 假设您有标签 y\n",
    "        # y = ...\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "        # clf = RandomForestClassifier()\n",
    "        # clf.fit(X_train, y_train)\n",
    "        # predictions = clf.predict(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yzhongenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
